{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo download de alguns módulos especificos do NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lucasqueiros/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lucasqueiros/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/lucasqueiros/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lucasqueiros/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization é o processo de transformar um texto em uma lista de tokens. Esses tokens podem ser senteças, palavras ou símbolos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = \"\"\"I sit back with this pack of Zig Zags and this bag.\n",
    "Of this weed it gives me the shit needed to be.\n",
    "The most meanest MC on this Earth. \n",
    "And since birth I've been cursed with this curse to just curse. \n",
    "And just blurt this berserk and bizarre shit that works.\n",
    "And it sells and it helps in itself to relieve.\n",
    "All this tension dispensing these sentences.\n",
    "Getting this stress that's been eating me recently off of this chest.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I sit back with this pack of Zig Zags and this bag.', 'Of this weed it gives me the shit needed to be.', 'The most meanest MC on this Earth.', \"And since birth I've been cursed with this curse to just curse.\", 'And just blurt this berserk and bizarre shit that works.', 'And it sells and it helps in itself to relieve.', 'All this tension dispensing these sentences.', \"Getting this stress that's been eating me recently off of this chest.\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = sent_tokenize(music)\n",
    "print(tokenized_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tambem podemos tokenizar em outras linguas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "musica = \"\"\"O que é, o que é?\n",
    "Clara e salgada.\n",
    "Cabe em um olho e pesa uma tonelada.\n",
    "Tem sabor de mar.\n",
    "Pode ser discreta.\n",
    "Inquilina da dor.\n",
    "Morada predileta.\n",
    "Na calada ela vem.\n",
    "Refém da vingança.\n",
    "Irmã do desespero.\n",
    "Rival da esperança.\n",
    "Pode ser causada por vermes e mundanas.\n",
    "E o espinho da flor.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O que é, o que é?', 'Clara e salgada.', 'Cabe em um olho e pesa uma tonelada.', 'Tem sabor de mar.', 'Pode ser discreta.', 'Inquilina da dor.', 'Morada predileta.', 'Na calada ela vem.', 'Refém da vingança.', 'Irmã do desespero.', 'Rival da esperança.', 'Pode ser causada por vermes e mundanas.', 'E o espinho da flor.'] portuguese\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(musica), \"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'sit', 'back', 'with', 'this', 'pack', 'of', 'Zig', 'Zags', 'and', 'this', 'bag', '.', 'Of', 'this', 'weed', 'it', 'gives', 'me', 'the', 'shit', 'needed', 'to', 'be', '.', 'The', 'most', 'meanest', 'MC', 'on', 'this', 'Earth', '.', 'And', 'since', 'birth', 'I', \"'ve\", 'been', 'cursed', 'with', 'this', 'curse', 'to', 'just', 'curse', '.', 'And', 'just', 'blurt', 'this', 'berserk', 'and', 'bizarre', 'shit', 'that', 'works', '.', 'And', 'it', 'sells', 'and', 'it', 'helps', 'in', 'itself', 'to', 'relieve', '.', 'All', 'this', 'tension', 'dispensing', 'these', 'sentences', '.', 'Getting', 'this', 'stress', 'that', \"'s\", 'been', 'eating', 'me', 'recently', 'off', 'of', 'this', 'chest', '.']\n",
      "----------------------------------------------------\n",
      "['O', 'que', 'é', ',', 'o', 'que', 'é', '?', 'Clara', 'e', 'salgada', '.', 'Cabe', 'em', 'um', 'olho', 'e', 'pesa', 'uma', 'tonelada', '.', 'Tem', 'sabor', 'de', 'mar', '.', 'Pode', 'ser', 'discreta', '.', 'Inquilina', 'da', 'dor', '.', 'Morada', 'predileta', '.', 'Na', 'calada', 'ela', 'vem', '.', 'Refém', 'da', 'vingança', '.', 'Irmã', 'do', 'desespero', '.', 'Rival', 'da', 'esperança', '.', 'Pode', 'ser', 'causada', 'por', 'vermes', 'e', 'mundanas', '.', 'E', 'o', 'espinho', 'da', 'flor', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(music)\n",
    "print(tokenized_word)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "tokenized_word_ptbr = word_tokenize(musica)\n",
    "print(word_tokenize(musica, \"portuguese\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stopwords sao palavras comuns que nao carregam muito significado contextual, e por isso, são frequentemente removidas durante o pré-processamento do texto.\n",
    "- Essas palavras incluem: artigos, preposições, conjunções, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'through', 'before', 'was', 'ma', 'the', \"it's\", 'up', 'so', 'over', 'or', 'their', 'no', 'mightn', 'mustn', 'be', 'shan', 'your', 'such', \"don't\", 'nor', 'now', 'itself', 've', 'had', 're', 'as', 'shouldn', 'hadn', \"shan't\", 'in', 'few', 'and', 'where', 'out', 'am', 'a', 'needn', 'theirs', 'ours', 'for', 'couldn', 'you', 'who', \"isn't\", 'we', 'it', 'an', 'than', 'any', 'themselves', 'to', 'yourself', 'only', 'hers', 'each', 'doing', 'did', 't', 'he', 'there', 'just', 'below', 'after', 'under', 'into', \"that'll\", 'above', 'o', 'weren', 'were', 'while', 'of', \"you'd\", 'himself', 'when', 'him', 'my', \"needn't\", 'both', 'from', 'because', 'will', 'all', 'having', 'these', 'once', 'about', 'me', 'm', 'didn', 'same', 'wouldn', 'does', 'own', \"won't\", \"wouldn't\", 'what', 'which', 'but', 's', 'how', 'haven', \"weren't\", 'll', 'those', \"doesn't\", 'they', 'doesn', 'is', 'between', 'do', 'isn', 'she', 'i', \"mustn't\", 'wasn', \"didn't\", \"you're\", 'has', 'not', 'herself', 'too', 'are', 'that', 'd', \"shouldn't\", 'at', 'if', \"you'll\", 'here', 'further', 'ain', 'with', 'off', 'our', 'yours', 'very', 'them', \"couldn't\", 'yourselves', \"you've\", 'most', \"aren't\", \"haven't\", 'should', \"hadn't\", \"should've\", 'ourselves', 'more', \"she's\", 'during', 'have', 'hasn', 'won', 'why', \"hasn't\", 'against', \"wasn't\", 'other', 'its', 'myself', 'some', 'this', 'until', 'his', 'whom', 'down', 'by', 'can', 'aren', 'on', 'been', 'again', \"mightn't\", 'then', 'being', 'don', 'y', 'her'}\n",
      "----------------------------------------------------\n",
      "{'haver', 'seus', 'delas', 'houverem', 'houveremos', 'estiver', 'teremos', 'hei', 'à', 'houveria', 'minhas', 'haja', 'no', 'tivera', 'houvemos', 'foi', 'também', 'terei', 'aquele', 'tiverem', 'aquelas', 'aquilo', 'esses', 'estava', 'tua', 'uma', 'teriam', 'temos', 'essas', 'será', 'as', 'você', 'deles', 'teve', 'a', 'dela', 'nos', 'esteja', 'isso', 'aos', 'fossem', 'só', 'terá', 'for', 'lhes', 'estiverem', 'ou', 'pelas', 'nossas', 'estejamos', 'nosso', 'numa', 'forem', 'estes', 'tenhamos', 'houveram', 'por', 'há', 'nas', 'meu', 'os', 'sejam', 'sejamos', 'tivessem', 'pelo', 'tenham', 'houveríamos', 'houvera', 'seria', 'esse', 'estas', 'houvéramos', 'hajam', 'mas', 'nossos', 'sem', 'das', 'estivemos', 'este', 'o', 'estávamos', 'teríamos', 'tivéssemos', 'fôssemos', 'tém', 'tenha', 'tuas', 'são', 'era', 'é', 'houve', 'nossa', 'estiveram', 'estão', 'me', 'havemos', 'tenho', 'depois', 'da', 'sua', 'eram', 'esta', 'houverá', 'nós', 'serão', 'não', 'na', 'estar', 'com', 'tinham', 'hão', 'somos', 'do', 'estou', 'meus', 'fora', 'houvessem', 'que', 'estejam', 'estivessem', 'estavam', 'se', 'um', 'fosse', 'já', 'muito', 'tem', 'isto', 'mais', 'fomos', 'teria', 'tive', 'tiveram', 'tivermos', 'fôramos', 'seu', 'houveriam', 'houvéssemos', 'e', 'seríamos', 'quem', 'ao', 'em', 'estivéramos', 'formos', 'seriam', 'ela', 'estivera', 'estamos', 'até', 'às', 'eles', 'hajamos', 'houvermos', 'mesmo', 'éramos', 'fui', 'tivemos', 'minha', 'estive', 'ser', 'está', 'estivermos', 'essa', 'entre', 'estivesse', 'houver', 'sou', 'eu', 'foram', 'houvesse', 'dele', 'lhe', 'como', 'teus', 'houverei', 'para', 'qual', 'houverão', 'tinha', 'tivesse', 'ele', 'de', 'estivéssemos', 'vos', 'aquela', 'dos', 'te', 'pela', 'seja', 'teu', 'pelos', 'tu', 'aqueles', 'tínhamos', 'esteve', 'seremos', 'vocês', 'elas', 'num', 'suas', 'tiver', 'serei', 'terão', 'nem', 'quando', 'tivéramos'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "stop_words_ptbr = set(stopwords.words(\"portuguese\"))\n",
    "print(stop_words_ptbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['I', 'sit', 'back', 'with', 'this', 'pack', 'of', 'Zig', 'Zags', 'and', 'this', 'bag', '.', 'Of', 'this', 'weed', 'it', 'gives', 'me', 'the', 'shit', 'needed', 'to', 'be', '.', 'The', 'most', 'meanest', 'MC', 'on', 'this', 'Earth', '.', 'And', 'since', 'birth', 'I', \"'ve\", 'been', 'cursed', 'with', 'this', 'curse', 'to', 'just', 'curse', '.', 'And', 'just', 'blurt', 'this', 'berserk', 'and', 'bizarre', 'shit', 'that', 'works', '.', 'And', 'it', 'sells', 'and', 'it', 'helps', 'in', 'itself', 'to', 'relieve', '.', 'All', 'this', 'tension', 'dispensing', 'these', 'sentences', '.', 'Getting', 'this', 'stress', 'that', \"'s\", 'been', 'eating', 'me', 'recently', 'off', 'of', 'this', 'chest', '.']\n",
      "Filtered Sentence: ['I', 'sit', 'back', 'pack', 'Zig', 'Zags', 'bag', '.', 'Of', 'weed', 'gives', 'shit', 'needed', '.', 'The', 'meanest', 'MC', 'Earth', '.', 'And', 'since', 'birth', 'I', \"'ve\", 'cursed', 'curse', 'curse', '.', 'And', 'blurt', 'berserk', 'bizarre', 'shit', 'works', '.', 'And', 'sells', 'helps', 'relieve', '.', 'All', 'tension', 'dispensing', 'sentences', '.', 'Getting', 'stress', \"'s\", 'eating', 'recently', 'chest', '.']\n",
      "----------------------------------------------------\n",
      "Palavras tokenizadas: ['O', 'que', 'é', ',', 'o', 'que', 'é', '?', 'Clara', 'e', 'salgada', '.', 'Cabe', 'em', 'um', 'olho', 'e', 'pesa', 'uma', 'tonelada', '.', 'Tem', 'sabor', 'de', 'mar', '.', 'Pode', 'ser', 'discreta', '.', 'Inquilina', 'da', 'dor', '.', 'Morada', 'predileta', '.', 'Na', 'calada', 'ela', 'vem', '.', 'Refém', 'da', 'vingança', '.', 'Irmã', 'do', 'desespero', '.', 'Rival', 'da', 'esperança', '.', 'Pode', 'ser', 'causada', 'por', 'vermes', 'e', 'mundanas', '.', 'E', 'o', 'espinho', 'da', 'flor', '.']\n",
      "Sentenças filtradas: ['O', ',', '?', 'Clara', 'salgada', '.', 'Cabe', 'olho', 'pesa', 'tonelada', '.', 'Tem', 'sabor', 'mar', '.', 'Pode', 'discreta', '.', 'Inquilina', 'dor', '.', 'Morada', 'predileta', '.', 'Na', 'calada', 'vem', '.', 'Refém', 'vingança', '.', 'Irmã', 'desespero', '.', 'Rival', 'esperança', '.', 'Pode', 'causada', 'vermes', 'mundanas', '.', 'E', 'espinho', 'flor', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in tokenized_word if word not in stop_words]\n",
    "print(\"Tokenized words:\", tokenized_word)\n",
    "print(\"Filtered Sentence:\", filtered_words)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "palavras_filtradas = [palavra for palavra in tokenized_word_ptbr if palavra not in stop_words_ptbr]\n",
    "print(\"Palavras tokenizadas:\", tokenized_word_ptbr)\n",
    "print(\"Sentenças filtradas:\", palavras_filtradas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Stemming reduz as palavras aos seus radicais. Por exemplo, as palavras connection, connected, connecting serão reduzidas a \"connect\".\n",
    "- Existem diversos algoritmos de Stemming, mas o mais famoso é o Porter stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['connect', 'connecting', 'connected']\n",
      "Stemmed Sentence: ['connect', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "example_words = ['connect', 'connecting', 'connected']\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_words = [ps.stem(w) for w in example_words]\n",
    "\n",
    "print(\"Filtered Sentence:\", example_words)\n",
    "print(\"Stemmed Sentence:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algoritmo SnowBall pode fazer stemming em até 13 línguas diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras Stemmed: ['conexã', 'conect', 'conect', 'conect']\n"
     ]
    }
   ],
   "source": [
    "exemplo_palavras = ['conexão', 'conectado', 'conectando', 'conectar']\n",
    "\n",
    "ss = SnowballStemmer(\"portuguese\")\n",
    "\n",
    "palavras_stemmed = [ss.stem(w) for w in exemplo_palavras]\n",
    "\n",
    "print('Palavras Stemmed:', palavras_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- O processo de Lematization reduz as palavras a sua forma base, conhecida como lemma. \n",
    "- Por exemplo, a palavra \"better\" tem \"good\" como sua lemma.\n",
    "- Em geral, é mais sofisticada que o stemming, pois leva em consideração o contexto.\n",
    "- É mais lento que o stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speak\n",
      "are\n",
      "gees\n",
      "went\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('stones'))\n",
    "print(stemmer.stem('speaking'))\n",
    "print(stemmer.stem('are'))\n",
    "print(stemmer.stem('geese'))\n",
    "print(stemmer.stem('went'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stone\n",
      "speak\n",
      "be\n",
      "goose\n",
      "go\n",
      "went\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('stones'))\n",
    "print(lemmatizer.lemmatize('speaking', pos='v'))\n",
    "print(lemmatizer.lemmatize('are', pos='v'))\n",
    "print(lemmatizer.lemmatize('geese'))\n",
    "print(lemmatizer.lemmatize('went', pos='v'))\n",
    "print(lemmatizer.lemmatize('went'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- aqui usamos o parametro \"pos='v'\" para especificar a classe gramatical da palavra onde 'v' significa verbo.\n",
    "- Na ultima linha usamos a palavra went sem passar 'v' como parametro e podemos ver que o lemmatize retornou o próprio went"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O principal objetivo de **Part-of-Speech (POS)** é identificar o grupo gramatical de uma certa palavra: *nome, pronome, adjetivo, verbo, advérbio, etc. Ela leva em consideração o contexto e procura por relacionamentos dentro da sentença e atribui uma tag correspondente a palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Albert', 'Einstein', 'was', 'born', 'in', 'Ulm', ',', 'Germany', 'in', '1879', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Ulm', 'NNP'),\n",
       " (',', ','),\n",
       " ('Germany', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1879', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print('Sentence:', tokens)\n",
    "\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequencias sobrepostas de n-palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'sit'), ('sit', 'back'), ('back', 'with'), ('with', 'this'), ('this', 'pack'), ('pack', 'of'), ('of', 'Zig'), ('Zig', 'Zags'), ('Zags', 'and'), ('and', 'this'), ('this', 'bag'), ('bag', '.'), ('.', 'Of'), ('Of', 'this'), ('this', 'weed'), ('weed', 'it'), ('it', 'gives'), ('gives', 'me'), ('me', 'the'), ('the', 'shit'), ('shit', 'needed'), ('needed', 'to'), ('to', 'be'), ('be', '.'), ('.', 'The'), ('The', 'most'), ('most', 'meanest'), ('meanest', 'MC'), ('MC', 'on'), ('on', 'this'), ('this', 'Earth'), ('Earth', '.'), ('.', 'And'), ('And', 'since'), ('since', 'birth'), ('birth', 'I'), ('I', \"'ve\"), (\"'ve\", 'been'), ('been', 'cursed'), ('cursed', 'with'), ('with', 'this'), ('this', 'curse'), ('curse', 'to'), ('to', 'just'), ('just', 'curse'), ('curse', '.'), ('.', 'And'), ('And', 'just'), ('just', 'blurt'), ('blurt', 'this'), ('this', 'berserk'), ('berserk', 'and'), ('and', 'bizarre'), ('bizarre', 'shit'), ('shit', 'that'), ('that', 'works'), ('works', '.'), ('.', 'And'), ('And', 'it'), ('it', 'sells'), ('sells', 'and'), ('and', 'it'), ('it', 'helps'), ('helps', 'in'), ('in', 'itself'), ('itself', 'to'), ('to', 'relieve'), ('relieve', '.'), ('.', 'All'), ('All', 'this'), ('this', 'tension'), ('tension', 'dispensing'), ('dispensing', 'these'), ('these', 'sentences'), ('sentences', '.'), ('.', 'Getting'), ('Getting', 'this'), ('this', 'stress'), ('stress', 'that'), ('that', \"'s\"), (\"'s\", 'been'), ('been', 'eating'), ('eating', 'me'), ('me', 'recently'), ('recently', 'off'), ('off', 'of'), ('of', 'this'), ('this', 'chest'), ('chest', '.')]\n",
      "-----------------------------------------\n",
      "[('O', 'que'), ('que', 'é'), ('é', ','), (',', 'o'), ('o', 'que'), ('que', 'é'), ('é', '?'), ('?', 'Clara'), ('Clara', 'e'), ('e', 'salgada'), ('salgada', '.'), ('.', 'Cabe'), ('Cabe', 'em'), ('em', 'um'), ('um', 'olho'), ('olho', 'e'), ('e', 'pesa'), ('pesa', 'uma'), ('uma', 'tonelada'), ('tonelada', '.'), ('.', 'Tem'), ('Tem', 'sabor'), ('sabor', 'de'), ('de', 'mar'), ('mar', '.'), ('.', 'Pode'), ('Pode', 'ser'), ('ser', 'discreta'), ('discreta', '.'), ('.', 'Inquilina'), ('Inquilina', 'da'), ('da', 'dor'), ('dor', '.'), ('.', 'Morada'), ('Morada', 'predileta'), ('predileta', '.'), ('.', 'Na'), ('Na', 'calada'), ('calada', 'ela'), ('ela', 'vem'), ('vem', '.'), ('.', 'Refém'), ('Refém', 'da'), ('da', 'vingança'), ('vingança', '.'), ('.', 'Irmã'), ('Irmã', 'do'), ('do', 'desespero'), ('desespero', '.'), ('.', 'Rival'), ('Rival', 'da'), ('da', 'esperança'), ('esperança', '.'), ('.', 'Pode'), ('Pode', 'ser'), ('ser', 'causada'), ('causada', 'por'), ('por', 'vermes'), ('vermes', 'e'), ('e', 'mundanas'), ('mundanas', '.'), ('.', 'E'), ('E', 'o'), ('o', 'espinho'), ('espinho', 'da'), ('da', 'flor'), ('flor', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "string_bigrams = list(bigrams(tokenized_word))\n",
    "print(string_bigrams)\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "string_bigramas = list(bigrams(tokenized_word_ptbr))\n",
    "print(string_bigramas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
