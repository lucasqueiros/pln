{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIq6MoiJ62f9"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfD96eaj8fzX"
      },
      "source": [
        "CountVectorizer tokeniza, cria o vocabulário e conta a frequência de ocorrência das palavras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8S5M159J6yDF"
      },
      "source": [
        "docs=[\"the house had a tiny little mouse\",\n",
        "      \"the cat saw the mouse\",\n",
        "      \"the mouse ran away from the house\",\n",
        "      \"the cat finally ate the mouse\",\n",
        "      \"the end of the mouse story\"\n",
        "     ]\n",
        "#instancia CountVectorizer()\n",
        "cv=CountVectorizer()\n",
        "#uma matriz esparsa é retornada com a contagem das palavras para cada documento.\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McCot1bU7YSu",
        "outputId": "3fb01010-8ac5-4f17-a0b1-6f455336e36c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count_vector.shape\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 16)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EGuGrOWFB4u",
        "outputId": "d5cb503d-45fe-40a6-87de-7c602a302753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(word_count_vector.toarray())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1]\n",
            " [0 0 1 0 0 0 0 0 0 1 0 0 1 0 2 0]\n",
            " [0 1 0 0 0 1 0 1 0 1 0 1 0 0 2 0]\n",
            " [1 0 1 0 1 0 0 0 0 1 0 0 0 0 2 0]\n",
            " [0 0 0 1 0 0 0 0 0 1 1 0 0 1 2 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01sfb0JHEbP0",
        "outputId": "20e4a14a-0b72-483e-b00a-87ea3e1f8606",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cv.get_feature_names_out()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ate', 'away', 'cat', 'end', 'finally', 'from', 'had', 'house',\n",
              "       'little', 'mouse', 'of', 'ran', 'saw', 'story', 'the', 'tiny'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au0SeDFy-UuP"
      },
      "source": [
        "É possível escoher o tokenizador, stopwords, n-gramas e muitos outros parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUr7Dbn8E7Vo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ac6ab4-fbc0-4921-8951-ed6176f2cf64"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('\\w+')\n",
        "\n",
        "cv=CountVectorizer()\n",
        "\n",
        "cv.set_params(tokenizer=tokenizer.tokenize)\n",
        "\n",
        "# remove stop words\n",
        "cv.set_params(stop_words='english')\n",
        "\n",
        "# considera 1-gramas e 2-gramas\n",
        "cv.set_params(ngram_range=(1, 2))\n",
        "\n",
        "# ignore terms that appear in more than 50% of the documents\n",
        "#vect.set_params(max_df=0.5)\n",
        "\n",
        "# only keep terms that appear in at least 2 documents\n",
        "#vect.set_params(min_df=2)\n",
        "\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnFFRDRaGcrJ",
        "outputId": "fb8ee561-68dd-4ff1-dd41-60422fbee323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc8srUyyA0kc",
        "outputId": "3d106dc1-45de-4f5b-c98e-e319c154f7d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cv.get_feature_names_out()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ate', 'ate mouse', 'away', 'away house', 'cat', 'cat finally',\n",
              "       'cat saw', 'end', 'end mouse', 'finally', 'finally ate', 'house',\n",
              "       'house tiny', 'little', 'little mouse', 'mouse', 'mouse ran',\n",
              "       'mouse story', 'ran', 'ran away', 'saw', 'saw mouse', 'story',\n",
              "       'tiny', 'tiny little'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Nu3DtqGmFH",
        "outputId": "a2cf84ce-0ea9-4ca0-e25d-86bbd80ddb03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_count_vector.toarray()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "        0, 0, 0],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
              "        0, 0, 0],\n",
              "       [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "        1, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6gptKPh-71H"
      },
      "source": [
        "TfidfVectorizer permite construir vetores TF*IDF para cada documento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3vsfszGm_u",
        "outputId": "9b30ab3a-f74c-4f8e-d23c-6e75fa1a85c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# instancia um objeto da classe TfidfVectorizer\n",
        "tfidf_vectorizer=TfidfVectorizer()\n",
        "\n",
        "# passa a lista de documentos para vetorização tf-idf\n",
        "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n",
        "\n",
        "tfidf_vectorizer_vectors.toarray()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.49356209, 0.39820278, 0.49356209, 0.23518498,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.23518498,\n",
              "        0.49356209],\n",
              "       [0.        , 0.        , 0.48334378, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.28547062,\n",
              "        0.        , 0.        , 0.59909216, 0.        , 0.57094124,\n",
              "        0.        ],\n",
              "       [0.        , 0.45709287, 0.        , 0.        , 0.        ,\n",
              "        0.45709287, 0.        , 0.36877965, 0.        , 0.2178072 ,\n",
              "        0.        , 0.45709287, 0.        , 0.        , 0.43561441,\n",
              "        0.        ],\n",
              "       [0.51392301, 0.        , 0.41462985, 0.        , 0.51392301,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24488707,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.48977413,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.49175319, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.23432303,\n",
              "        0.49175319, 0.        , 0.        , 0.49175319, 0.46864606,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlgS3vfWTSp8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}